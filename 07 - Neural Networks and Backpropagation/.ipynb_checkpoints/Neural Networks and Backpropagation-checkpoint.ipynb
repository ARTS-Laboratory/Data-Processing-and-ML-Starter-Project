{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e12f495",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "    Neural Networks and Backpropagation\n",
    "    \n",
    "    Author: Daniel Coble\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6cf0b",
   "metadata": {},
   "source": [
    "We now know enough to talk about multilayer perceptrons and and neural networks. As we saw in the gradient descent notebook, gradient descent is the normal algorithm for how a machine 'learns'. But for gradient descent to work, there needs to be an algorithm to efficiently calculate gradients. That algorithm is backpropagation. This notebook will cover neural networks and how backpropagation through neural networks works, and implementing an MLP by hand. That'll involve some in-depth calculation, which is annoying. When doing actual machine learning, we don't have to do that and can just use an ML library.\n",
    "But it's good to have an understanding of what's going on under the hood. Then we can move on and never have to worry about backpropagation ever again.\n",
    "\n",
    "A neural network layer is a transform from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and consists of an affine transform (linear transform with a bias), then an activation function applied elementwise to the vector.\n",
    "$$ \\text{Layer = affine transform + activation function} $$\n",
    "$$\\phi(x) = \\sigma(Wx+b) $$\n",
    "\n",
    "The matrix and vector $W$ and $b$ are the weights and bias of the NN layer (sometimes they are both called weights) and parameterize the function $\\phi$. $W$ and $b$ are like the $\\beta$ in linear regression, and it's the goal of training to find the optimal weights. A single-layer NN isn't too useful, rather they are stacked on top of each other to make (hence the name 'layer'). Consider the layer functions $\\phi_1: \\mathbb{R}^{i} \\rightarrow \\mathbb{R}^{h_1}$, $\\phi_2: \\mathbb{R}^{h_1} \\rightarrow \\mathbb{R}^{h_2}$, $\\phi_3: \\mathbb{R}^{h_2} \\rightarrow \\mathbb{R}$. A 3-layer MLP is\n",
    "$$ \\phi(x) = \\phi_3(\\phi_2(\\phi_1(x))) = \\sigma\\left(W_3\\sigma\\left(W_2\\sigma\\left(W_1x+b_1\\right) + b_2\\right)+b_3\\right) $$\n",
    "and the weights of the entire NN which need to be optimized are $\\mathbb{W} = \\{W_1,W_2,W_3,b_1,b_2,b_3\\}$. For future reference, let's also define the following vectors. \n",
    "$$\\xi_1 = W_1x + b_1$$\n",
    "$$ \\xi_2 = W_2\\phi_1 + b_2 $$\n",
    "$$ \\xi_3 = W_3\\phi_2 + b_2 $$\n",
    "For simplicity I will surpress the dependence of $\\phi_1$, $\\phi_2$, and $\\phi_3$. We can now work through how the gradient of loss with respect to the weights is calculated with backpropagation. Let a dataset be $\\{(x_i, y_i)\\}_{i=1}^N$. Loss is mean squared error.\n",
    "$$ L(\\mathbb{W}) = \\frac{1}{N}\\sum_{i=1}^N \\left(\\phi(x_i)-y_i\\right)^2 $$\n",
    "Backpropagation is called that because it works in the opposite direction of inference: where in inference $\\phi_1$ is caculated, then $\\phi_2$, then $\\phi_3$, in backpropagation, the gradient with respect to the weights of $\\phi_3$ is calculated first, then $\\phi_2$, then $\\phi_1$. The principle of backpropagation is the chain rule. As the first step, we have to calculate the gradients $\\frac{\\partial L}{\\partial W_3}$, $\\frac{\\partial L}{\\partial b_3}$, and $\\frac{\\partial L}{\\partial \\phi_2}$. The first two are used for weight updating and the last is used to continue backpropagation. As convention, we'll have the numerator of the partial derivative always be a scalar, and if the denominator is a matrix or vector, take that as short hand for the matrix/vector of the same shape of the partial derivative of each element. I'll develop the notation $\\nabla_x y = \\left[\\frac{\\partial y_1}{\\partial x_1},...,\\frac{\\partial y_n}{\\partial x_n}\\right]^T$ for vectors $x$ and $y$ to be an elementwise application of the partial derivative.\n",
    "$$ \\frac{\\partial L}{\\partial W_3} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial W_3} = \\sum_{i=1}^N\\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial \\xi_3}\\frac{\\partial \\xi_3}{\\partial W_3}$$\n",
    "$$ \\frac{\\partial L}{\\partial b_3} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial b_3} = \\sum_{i=1}^N\\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial \\xi_3}\\frac{\\partial \\xi_3}{\\partial b_3}$$\n",
    "$$ \\frac{\\partial L}{\\partial \\phi_2} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial \\phi_2} = \\sum_{i=1}^N\\frac{\\partial L}{\\partial \\phi_3}\\frac{\\partial \\phi_3}{\\partial \\xi_3}\\frac{\\partial \\xi_3}{\\partial \\phi_2}$$\n",
    "We can look at each of those partial derivatives separately. You can satisfy yourself that these are the correct equations.\n",
    "$$ \\frac{\\partial L}{\\partial \\phi_3} = \\frac{2}{N}\\left(\\phi_3-y_i\\right) $$\n",
    "$$ \\frac{\\partial \\phi_3}{\\partial \\xi_3} = \\sigma'(\\xi_3)$$\n",
    "$$ \\frac{\\partial \\xi_3}{\\partial W_3} = \\phi_2^T$$\n",
    "$$ \\frac{\\partial \\xi_3}{\\partial b_3} =  1 $$\n",
    "$$ \\frac{\\partial \\xi_3}{\\partial \\phi_2} = W_3^T $$\n",
    "That solves the third layer, and we can continue to solving the second layer. The process is very similar except we have to be careful that $\\phi_2$ is a vector.\n",
    "$$ \\frac{\\partial L}{\\partial W_2} =  \\sum_{i=1}^N \\sum_{j=1}^{h_2} \\frac{\\partial L}{\\partial \\phi_{2,j}}\\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}}\\frac{\\partial\\xi_{2,j}}{\\partial W_2}$$\n",
    "$$ \\frac{\\partial L}{\\partial b_2} = \\sum_{i=1}^N \\sum_{j=1}^{h_2} \\frac{\\partial L}{\\partial \\phi_{2,j}}\\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}}\\frac{\\partial\\xi_{2,j}}{\\partial b_2} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\phi_1} = \\sum_{i=1}^N \\sum_{j=1}^{h_2} \\frac{\\partial L}{\\partial \\phi_{2,j}}\\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}}\\frac{\\partial\\xi_{2,j}}{\\partial \\phi_1} $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W_2} = \\sum_{i=1}^N\\frac{\\partial L}{\\partial \\phi_{2}}\\circ\\nabla_{\\xi_2}\\phi_2 \\frac{\\partial\\xi_{2,j}}{\\partial W_2}$$\n",
    "$$ \\frac{\\partial L}{\\partial b_2} = \\sum_{i=1}^N \\sum_{j=1}^{h_2} \\frac{\\partial L}{\\partial \\phi_{2,j}}\\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}}\\frac{\\partial\\xi_{2,j}}{\\partial b_2} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\phi_1} = \\sum_{i=1}^N \\sum_{j=1}^{h_2} \\frac{\\partial L}{\\partial \\phi_{2,j}}\\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}}\\frac{\\partial\\xi_{2,j}}{\\partial \\phi_1} $$\n",
    "You can check that the following equations are correct.\n",
    "$$ \\frac{\\partial \\phi_{2,j}}{\\partial \\xi_{2,j}} = \\sigma'(\\xi_{2,j})$$\n",
    "$$ \\frac{\\partial\\xi_{2,j}}{\\partial W_2} = $$\n",
    "$$ \\frac{\\partial\\xi_{2,j}}{\\partial b_2} = $$\n",
    "$$ \\frac{\\partial\\xi_{2,j}}{\\partial \\phi_1} = $$\n",
    "\n",
    "This is a mess and frankly it sucks. On the bright side it's actually very nice. The computational cost of calculating the gradients is on the same order as doing the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1872e",
   "metadata": {},
   "source": [
    "**Challenge Problem**\n",
    "\n",
    "The code above is hard-coded for a 3-layer MLP. Refactor it so that you can create a NN of arbitrarily many layers. Experiment with changing the number of layers and hidden units in the datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
