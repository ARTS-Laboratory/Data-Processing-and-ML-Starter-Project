{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e12f495",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "    Neural Networks and Backpropagation\n",
    "    \n",
    "    Author: Daniel Coble\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6cf0b",
   "metadata": {},
   "source": [
    "We now know enough to talk about multilayer perceptrons and and neural networks. As we saw in the gradient descent notebook, gradient descent is the normal algorithm for how a machine 'learns'. But for gradient descent to work, there needs to be an algorithm to efficiently calculate gradients. That algorithm is backpropagation. This notebook will cover neural networks and how backpropagation through neural networks works, and implementing an MLP by hand. That'll involve some in-depth calculation, which is annoying. When doing actual machine learning, we don't have to do that and can just use an ML library.\n",
    "But it's good to have an understanding of what's going on under the hood. Then we can move on and never have to worry about backpropagation ever again.\n",
    "\n",
    "A neural network layer is a transform from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and consists of an affine transform (linear transform with a bias), then an activation function applied elementwise to the vector.\n",
    "$$ \\text{Layer = affine transform + activation function} $$\n",
    "$$\\phi(x) = \\sigma(Wx+b) $$\n",
    "\n",
    "The matrix and vector $W$ and $b$ are the weights and bias of the NN layer (sometimes they are both called weights) and parameterize the function $\\phi$. $W$ and $b$ are like the $\\beta$ in linear regression, and it's the goal of training to find the optimal weights. A single-layer NN isn't too useful, rather they are stacked on top of each other to make (hence the name 'layer'). Consider the layer functions $\\phi_1: \\mathbb{R}^{i} \\rightarrow \\mathbb{R}^{h_1}$, $\\phi_2: \\mathbb{R}^{h_1} \\rightarrow \\mathbb{R}^{h_2}$, $\\phi_3: \\mathbb{R}^{h_2} \\rightarrow \\mathbb{R}$. A 3-layer MLP is\n",
    "$$ \\phi(x) = \\phi_3(\\phi_2(\\phi_1(x))) = \\sigma\\left(W_3\\phi_2\\left(\\right)+b_3\\right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1872e",
   "metadata": {},
   "source": [
    "**Challenge Problem**\n",
    "\n",
    "The code above is hard-coded for a 3-layer MLP. Refactor it so that you can create a NN of arbitrarily many layers. Experiment with changing the number of layers and hidden units in the datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
